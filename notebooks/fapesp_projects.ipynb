{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de bibliotecas\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicao da raiz do projeto\n",
    "\n",
    "PROJECT_ROOT = 'G:/Csouza/nlp/topic_modeling'\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "sys.path.insert(0, PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(extract_path, file_name='all_process.xlsx', sheet_name='Sheet1'):\n",
    "    \n",
    "    return pl.read_excel(f'{extract_path}/{file_name}', sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(PROJECT_ROOT, 'data', 'internal', 'fapesp_projects')\n",
    "\n",
    "full_data = extract(data_path)\n",
    "\n",
    "variables = {\n",
    "'N. Processo_B.V': 'n_processo',\n",
    "'Data de Início': 'data',\n",
    "'Título (Português)': 'titulo',\n",
    "'Grande Área do Conhecimento': 'grande_area',\n",
    "'Área do Conhecimento': 'area',\n",
    "'Subárea do Conhecimento': 'subarea',\n",
    "'Palavras-Chave do Processo': 'palavras_chave',\n",
    "'Assuntos': 'assuntos',\n",
    "'Resumo (Português)': 'resumo'}\n",
    "\n",
    "full_data = (\n",
    "    full_data\n",
    "    .lazy()\n",
    "    .rename(variables)\n",
    "    .select(variables.values())\n",
    "    .filter(\n",
    "        pl.col('n_processo').is_not_null(),\n",
    "        pl.col('resumo').is_not_null(),\n",
    "        pl.col('resumo') != '')\n",
    "    .with_columns(\n",
    "        pl.col('data').str.to_datetime('%m-%d-%y').dt.year().alias('ano'),\n",
    "        pl.col('data').str.to_datetime('%m-%d-%y').dt.month().alias('mes'))\n",
    "    .select(pl.exclude('data'))\n",
    ").collect()\n",
    "\n",
    "full_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_test = full_data.filter(pl.col('assuntos').is_not_null(), pl.col('area') == 'Medicina', pl.col('ano') >= 2022)\n",
    "\n",
    "data_train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_model(model='en_core_web_sm'):\n",
    "    \"\"\"\n",
    "    Baixa o modelo de linguagem spaCy se não estiver presente.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nlp = spacy.load(model)\n",
    "    except OSError:\n",
    "        from spacy.cli import download\n",
    "        download(model)\n",
    "        nlp = spacy.load(model)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o modelo de linguagem em português do spaCy\n",
    "nlp = get_spacy_model('pt_core_news_sm')\n",
    "\n",
    "# Definir as stop words em português usando spaCy\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# Compilador para remover caracteres especiais (exceto acentos e espaços)\n",
    "special_char_remover = re.compile(r'[^A-Za-zÀ-ÿ\\s]')\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(\"O argumento 'text' deve ser uma string.\")\n",
    "    \n",
    "    # Remover caracteres especiais\n",
    "    text = special_char_remover.sub('', text)\n",
    "    \n",
    "    # Tokenizar o texto e remover stop words\n",
    "    tokens = [token.text for token in nlp(text) if token.text not in stop_words]\n",
    "    \n",
    "    # Lematizar o texto\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    text = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlsmote(X, y, num_samples=100, minority_threshold=5):\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "\n",
    "    # Identificar as instâncias minoritárias com base no limiar\n",
    "    label_counts = np.sum(y, axis=0)\n",
    "    print(\"Contadores de rótulos:\", label_counts)  # Depuração\n",
    "\n",
    "    minority_classes = np.where(label_counts < minority_threshold)[0]\n",
    "    print(\"Classes minoritárias:\", minority_classes)  # Depuração\n",
    "\n",
    "    # Selecionar exemplos minoritários\n",
    "    minority_indices = [i for i in range(len(y)) if any(y[i, minority_classes])]\n",
    "    print(\"Índices minoritários:\", minority_indices)  # Depuração\n",
    "\n",
    "    if len(minority_indices) == 0:\n",
    "        raise ValueError(\"Nenhum exemplo minoritário foi encontrado. Verifique os dados de entrada.\")\n",
    "\n",
    "    minority_X = X[minority_indices]\n",
    "    minority_y = y[minority_indices]\n",
    "\n",
    "    if len(minority_X) < 2:\n",
    "        raise ValueError(\"Exemplos minoritários insuficientes para aplicar MLSMOTE. Verifique os dados de entrada.\")\n",
    "\n",
    "    # Usar NearestNeighbors para encontrar os vizinhos mais próximos\n",
    "    nn = NearestNeighbors(n_neighbors=5).fit(minority_X)\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        idx = np.random.choice(range(len(minority_X)))\n",
    "        sample_X = minority_X[idx]\n",
    "        sample_y = minority_y[idx]\n",
    "        \n",
    "        neighbors = nn.kneighbors([sample_X], return_distance=False)[0]\n",
    "        neighbor_idx = neighbors[np.random.randint(1, len(neighbors))]\n",
    "        neighbor_X = minority_X[neighbor_idx]\n",
    "        neighbor_y = minority_y[neighbor_idx]\n",
    "        \n",
    "        new_sample_X = sample_X + np.random.rand() * (neighbor_X - sample_X)\n",
    "        new_sample_y = np.clip(sample_y + neighbor_y, 0, 1)\n",
    "\n",
    "        # Verificação de exemplos gerados\n",
    "        print(f\"Exemplo gerado - new_sample_X: {new_sample_X}, new_sample_y: {new_sample_y}\")\n",
    "        \n",
    "        new_X.append(new_sample_X)\n",
    "        new_y.append(new_sample_y)\n",
    "    \n",
    "    X_resampled = np.vstack([X, np.array(new_X)])\n",
    "    y_resampled = np.vstack([y, np.array(new_y)])\n",
    "\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados\n",
    "data = data_train_test.to_pandas()\n",
    "\n",
    "data['titulo'] = data['titulo'].astype(str)\n",
    "data['palavras_chave'] = data['palavras_chave'].astype(str)\n",
    "\n",
    "data['cleaned_text'] = data['resumo'].apply(clean_text)\n",
    "data['cleaned_text'] += ' Título: ' + data['titulo'].apply(clean_text) + ' Palavras-chave: ' + data['palavras_chave'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir os dados em treino e teste\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenização com BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "train_tokenized_texts = tokenizer(train_data['cleaned_text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_tokenized_texts = tokenizer(test_data['cleaned_text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Converte os assuntos em listas\n",
    "train_data['assuntos_list'] = train_data['assuntos'].apply(lambda x: x.split(':'))\n",
    "test_data['assuntos_list'] = test_data['assuntos'].apply(lambda x: x.split(':'))\n",
    "\n",
    "# Binariza os rótulos\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "train_binary_labels = mlb.fit_transform(train_data['assuntos_list'])\n",
    "test_binary_labels = mlb.transform(test_data['assuntos_list'])\n",
    "\n",
    "mean_exemples = np.median(np.sum(train_binary_labels, axis=0)).round()\n",
    "# Aplicar MLSMOTE aos dados de treinamento\n",
    "X_resampled, y_resampled = mlsmote(train_tokenized_texts['input_ids'].numpy(), train_binary_labels, num_samples=100, minority_threshold=mean_exemples)\n",
    "\n",
    "train_tokenized_texts['input_ids'] = torch.tensor(X_resampled, dtype=torch.long)\n",
    "train_binary_labels = torch.tensor(y_resampled, dtype=torch.float)\n",
    "\n",
    "# Criação de novos tensores para os dados resampled com atenção correta\n",
    "attention_mask = (X_resampled != 0).astype(int)  # Gera a máscara de atenção com base nos tokens não nulos\n",
    "\n",
    "train_tokenized_texts_resampled = {\n",
    "    'input_ids': torch.tensor(X_resampled, dtype=torch.long),\n",
    "    'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n",
    "}\n",
    "\n",
    "train_binary_labels_resampled = torch.tensor(y_resampled, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo BERT pré-treinado\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(mlb.classes_))\n",
    "\n",
    "# Ajuste da camada de classificação para multi-rótulo\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(model.config.hidden_size, model.config.hidden_size),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(model.config.hidden_size, len(mlb.classes_)),\n",
    "    torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congelar todas as camadas do BERT\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Descongelar a última camada\n",
    "for param in model.bert.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# A camada de classificação é treinada por padrão\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Função de perda e otimizador\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5, eps=1e-8)  # Apenas atualiza os parâmetros que requerem gradiente\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "# Scheduler de taxa de aprendizado\n",
    "total_steps = len(train_tokenized_texts['input_ids']) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Criação do DataLoader para os dados resampled\n",
    "train_dataset_resampled = CustomDataset(train_tokenized_texts_resampled, train_binary_labels_resampled)\n",
    "test_dataset = CustomDataset(test_tokenized_texts, torch.tensor(test_binary_labels, dtype=torch.float))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset_resampled, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.logits.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(dataloader), np.array(all_labels), np.array(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular métricas\n",
    "def compute_metrics(labels, preds, threshold=0.5):\n",
    "    preds = (preds > threshold).astype(int)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='samples')\n",
    "    recall = recall_score(labels, preds, average='samples')\n",
    "    precision = precision_score(labels, preds, average='samples')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'recall': recall,\n",
    "        'precision': precision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(PROJECT_ROOT, 'models')\n",
    "model_name = f'{model_path}/best_model.pt'\n",
    "\n",
    "if not os.path.exists(model_name):\n",
    "    # Loop de Treinamento e Avaliação\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    num_epochs = 3\n",
    "    best_f1 = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        \n",
    "        train_loss = train(model, train_dataloader, criterion, optimizer, scheduler, device)\n",
    "        print(f'Training loss: {train_loss:.4f}')\n",
    "        \n",
    "        val_loss, val_labels, val_preds = evaluate(model, test_dataloader, criterion, device)\n",
    "        print(f'Validation loss: {val_loss:.4f}')\n",
    "        \n",
    "        metrics = compute_metrics(np.array(val_labels), np.array(val_preds))\n",
    "        print(f'Validation metrics: {metrics}')\n",
    "        \n",
    "        if metrics['f1'] > best_f1:\n",
    "            print(f'Saving best model with F1 score: {metrics[\"f1\"]:.4f}')\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            best_f1 = metrics['f1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o modelo fine-tuned\n",
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()\n",
    "\n",
    "# Função para gerar embeddings\n",
    "def get_embeddings(texts, model, tokenizer):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.bert(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token embeddings\n",
    "    return embeddings\n",
    "\n",
    "# Gerar embeddings\n",
    "embeddings = get_embeddings(test_data['cleaned_text'].tolist(), model, tokenizer)\n",
    "\n",
    "# Verificar se os embeddings foram gerados corretamente\n",
    "if embeddings.size(0) == 0:\n",
    "    raise ValueError(\"Os embeddings gerados estão vazios.\")\n",
    "\n",
    "# Debug print para verificar os embeddings\n",
    "print(f\"Shape of embeddings: {embeddings.shape}\")\n",
    "\n",
    "# Inicializar BERTopic com um modelo de embeddings padrão\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "umap_model = UMAP(n_neighbors=3, n_components=2, metric='cosine', random_state=42)  # Ajuste n_neighbors conforme necessário\n",
    "topic_model = BERTopic(embedding_model=embedding_model, umap_model=umap_model)\n",
    "\n",
    "# Ajustar o modelo aos dados\n",
    "try:\n",
    "    topics, probabilities = topic_model.fit_transform(test_data['cleaned_text'].tolist(), embeddings.numpy())\n",
    "except ValueError as e:\n",
    "    raise ValueError(f\"Erro ao ajustar o modelo BERTopic: {e}\")\n",
    "\n",
    "# Verificar se os tópicos foram gerados corretamente\n",
    "if len(topics) == 0:\n",
    "    raise ValueError(\"Nenhum tópico foi gerado. Verifique os dados de entrada e os embeddings.\")\n",
    "\n",
    "# Debug print para verificar os tópicos\n",
    "print(f\"Number of topics: {len(set(topics))}\")\n",
    "\n",
    "# Verificar se os embeddings dos tópicos não são vazios antes da visualização\n",
    "if topic_model.topic_embeddings_ is not None:\n",
    "    if topic_model.topic_embeddings_.size == 0:\n",
    "        raise ValueError(\"Os embeddings dos tópicos estão vazios.\")\n",
    "else:\n",
    "    if topic_model.c_tf_idf_.size == 0:\n",
    "        raise ValueError(\"A matriz c_tf_idf_ dos tópicos está vazia.\")\n",
    "\n",
    "# Adicionar verificação de tamanho dos embeddings antes de visualizar os tópicos\n",
    "if topic_model.topic_embeddings_ is not None:\n",
    "    if topic_model.topic_embeddings_.shape[0] == 0 or topic_model.topic_embeddings_.shape[1] == 0:\n",
    "        raise ValueError(\"Os embeddings dos tópicos têm tamanho zero.\")\n",
    "else:\n",
    "    if topic_model.c_tf_idf_.shape[0] == 0 or topic_model.c_tf_idf_.shape[1] == 0:\n",
    "        raise ValueError(\"A matriz c_tf_idf_ dos tópicos tem tamanho zero.\")\n",
    "\n",
    "# Debug print para verificar os embeddings dos tópicos\n",
    "print(f\"Shape of topic embeddings: {topic_model.topic_embeddings_.shape if topic_model.topic_embeddings_ is not None else 'N/A'}\")\n",
    "print(f\"Shape of c_tf_idf_: {topic_model.c_tf_idf_.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_get_embeddings(texts, bert_model, tokenizer, device='cpu'):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs, output_hidden_states=True)\n",
    "    hidden_states = outputs.hidden_states[-2]\n",
    "    embeddings = hidden_states.mean(dim=1).cpu().numpy()  # Média dos embeddings e conversão para numpy array\n",
    "    return embeddings\n",
    "\n",
    "def batch_predict_labels(texts, model, tokenizer, threshold=0.5, top_k=None, device='cpu'):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "\n",
    "    preds = []\n",
    "    for prob in probs:\n",
    "        if top_k:\n",
    "            top_indices = np.argsort(prob)[-top_k:]  # Obter os índices dos top_k rótulos mais prováveis\n",
    "            pred = np.zeros(prob.shape)\n",
    "            pred[top_indices] = 1\n",
    "        else:\n",
    "            pred = (prob > threshold).astype(int)\n",
    "        preds.append(pred)\n",
    "    \n",
    "    return np.array(preds), probs\n",
    "\n",
    "def apply_bertopic(texts, topic_model, bert_model=None, tokenizer=None, use_bert_embeddings=False, device='cpu', top_n_topics=None, top_n_tokens=None):\n",
    "    if use_bert_embeddings and bert_model and tokenizer:\n",
    "        embeddings = batch_get_embeddings(texts, bert_model, tokenizer, device=device)\n",
    "        topics, _ = topic_model.transform(texts, embeddings=embeddings)  # Passar os embeddings aqui\n",
    "    else:\n",
    "        topics, _ = topic_model.transform(texts)\n",
    "    \n",
    "    topic_details = []\n",
    "    for topic in topics:\n",
    "        if topic != -1:\n",
    "            topic_tokens = topic_model.get_topic(topic)[:top_n_tokens] if top_n_tokens else topic_model.get_topic(topic)\n",
    "            topic_details.append((topic, topic_tokens))\n",
    "    \n",
    "    return topic_details[:top_n_topics] if top_n_topics else topic_details\n",
    "\n",
    "def consolidate_results(texts, model, tokenizer, topic_model, mlb, threshold=0.5, top_k=None, use_bert_embeddings=False, device='cpu', top_n_topics=None, top_n_tokens=None):\n",
    "    preds, probs = batch_predict_labels(texts, model, tokenizer, threshold, top_k, device)\n",
    "    known_labels = mlb.inverse_transform(preds)\n",
    "    \n",
    "    all_labels = []\n",
    "    topic_details = []\n",
    "\n",
    "    for i, (pred, prob) in enumerate(zip(preds, probs)):\n",
    "        if np.max(prob) < threshold:\n",
    "            topics = apply_bertopic([texts[i]], topic_model, bert_model=model, tokenizer=tokenizer, use_bert_embeddings=use_bert_embeddings, device=device, top_n_topics=top_n_topics, top_n_tokens=top_n_tokens)\n",
    "            topic_labels = [token for topic, tokens in topics for token in tokens]\n",
    "            all_labels.append(list(set(known_labels[i]) | set(topic_labels)))\n",
    "            topic_details.append(topics)\n",
    "        else:\n",
    "            all_labels.append(known_labels[i])\n",
    "            topic_details.append([])\n",
    "\n",
    "    return all_labels, topic_details\n",
    "\n",
    "def batch_predict(texts, model, tokenizer, topic_model, mlb, threshold=0.5, top_k=None, use_bert_embeddings=False, device='cpu', top_n_topics=None, top_n_tokens=None):\n",
    "    # Move model to device and set to evaluation mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    if use_bert_embeddings:\n",
    "        bert_model = model\n",
    "        bert_model.to(device)\n",
    "        bert_model.eval()\n",
    "    \n",
    "    all_labels, topic_details = consolidate_results(texts, model, tokenizer, topic_model, mlb, threshold, top_k, use_bert_embeddings, device, top_n_topics, top_n_tokens)\n",
    "    return list(zip(all_labels, topic_details))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = test_data['cleaned_text'].tolist()[0:4]\n",
    "\n",
    "threshold = 0.7\n",
    "top_k = 5\n",
    "top_n_topics = 3\n",
    "top_n_tokens = 5\n",
    "\n",
    "# Aplicar a predição em lote\n",
    "predicted_labels_and_topics = batch_predict(\n",
    "    texts=test_texts,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    topic_model=topic_model,\n",
    "    mlb=mlb,\n",
    "    threshold=threshold,\n",
    "    top_k=top_k,\n",
    "    use_bert_embeddings=False,  # Escolher se deseja usar embeddings do BERT\n",
    "    device='cpu',  # Ou 'cuda' se você estiver usando GPU\n",
    "    top_n_topics=top_n_topics,\n",
    "    top_n_tokens=top_n_tokens\n",
    ")\n",
    "\n",
    "# Exibir os resultados\n",
    "for text, (labels, topics) in zip(test_texts, predicted_labels_and_topics):\n",
    "    print(f\"Texto: {text}\")\n",
    "    print(f\"Rótulos preditos: {labels}\")\n",
    "    if topics:\n",
    "        print(\"Tópicos e tokens associados:\")\n",
    "        for topic, tokens in topics:\n",
    "            print(f\"  Tópico {topic}: {tokens}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
