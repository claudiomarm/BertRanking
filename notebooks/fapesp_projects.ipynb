{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\csouza\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\academic-fingerprint-4AictD0y-py3.11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importação de bibliotecas\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from bertopic import BERTopic\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicao da raiz do projeto\n",
    "\n",
    "PROJECT_ROOT = 'G:/Csouza/nlp/topic_modeling'\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "sys.path.insert(0, PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(extract_path, file_name='all_process.xlsx', sheet_name='Sheet1'):\n",
    "    \n",
    "    return pl.read_excel(f'{extract_path}/{file_name}', sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>n_processo</th><th>titulo</th><th>grande_area</th><th>area</th><th>subarea</th><th>palavras_chave</th><th>assuntos</th><th>resumo</th><th>ano</th><th>mes</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i32</td><td>i8</td></tr></thead><tbody><tr><td>&quot;95/04916-0&quot;</td><td>&quot;Estudo sistemático de campos h…</td><td>&quot;Ciências Exatas e da Terra&quot;</td><td>&quot;Física&quot;</td><td>&quot;Física da Matéria Condensada&quot;</td><td>&quot;CORRELACAO ANGULAR, ESTUDO SIS…</td><td>null</td><td>&quot;Este projeto está vinculado ao…</td><td>1995</td><td>12</td></tr><tr><td>&quot;95/05064-7&quot;</td><td>&quot;Cultura, ideologia e represent…</td><td>&quot;Ciências Humanas&quot;</td><td>&quot;Sociologia&quot;</td><td>&quot;Outras Sociologias Específicas&quot;</td><td>&quot;BRASIL, IDENTIDADE, PENSAMENTO…</td><td>&quot;Brasil:Identidade social&quot;</td><td>&quot;Participar do Seminário &quot;&quot;Soci…</td><td>1995</td><td>12</td></tr><tr><td>&quot;95/09836-4&quot;</td><td>&quot;Bernard Schmitt | Université d…</td><td>&quot;Ciências Exatas e da Terra&quot;</td><td>&quot;Probabilidade e Estatística&quot;</td><td>&quot;Probabilidade&quot;</td><td>&quot;COMPRESSOR, ENTROPIA, ESTADO D…</td><td>&quot;Entropia (matemática aplicada)…</td><td>&quot;O principal objetivo da visita…</td><td>1995</td><td>12</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 10)\n",
       "┌────────────┬─────────────┬─────────────┬─────────────┬───┬─────────────┬────────────┬──────┬─────┐\n",
       "│ n_processo ┆ titulo      ┆ grande_area ┆ area        ┆ … ┆ assuntos    ┆ resumo     ┆ ano  ┆ mes │\n",
       "│ ---        ┆ ---         ┆ ---         ┆ ---         ┆   ┆ ---         ┆ ---        ┆ ---  ┆ --- │\n",
       "│ str        ┆ str         ┆ str         ┆ str         ┆   ┆ str         ┆ str        ┆ i32  ┆ i8  │\n",
       "╞════════════╪═════════════╪═════════════╪═════════════╪═══╪═════════════╪════════════╪══════╪═════╡\n",
       "│ 95/04916-0 ┆ Estudo      ┆ Ciências    ┆ Física      ┆ … ┆ null        ┆ Este       ┆ 1995 ┆ 12  │\n",
       "│            ┆ sistemático ┆ Exatas e da ┆             ┆   ┆             ┆ projeto    ┆      ┆     │\n",
       "│            ┆ de campos   ┆ Terra       ┆             ┆   ┆             ┆ está       ┆      ┆     │\n",
       "│            ┆ h…          ┆             ┆             ┆   ┆             ┆ vinculado  ┆      ┆     │\n",
       "│            ┆             ┆             ┆             ┆   ┆             ┆ ao…        ┆      ┆     │\n",
       "│ 95/05064-7 ┆ Cultura,    ┆ Ciências    ┆ Sociologia  ┆ … ┆ Brasil:Iden ┆ Participar ┆ 1995 ┆ 12  │\n",
       "│            ┆ ideologia e ┆ Humanas     ┆             ┆   ┆ tidade      ┆ do         ┆      ┆     │\n",
       "│            ┆ represent…  ┆             ┆             ┆   ┆ social      ┆ Seminário  ┆      ┆     │\n",
       "│            ┆             ┆             ┆             ┆   ┆             ┆ \"\"Soci…    ┆      ┆     │\n",
       "│ 95/09836-4 ┆ Bernard     ┆ Ciências    ┆ Probabilida ┆ … ┆ Entropia    ┆ O          ┆ 1995 ┆ 12  │\n",
       "│            ┆ Schmitt |   ┆ Exatas e da ┆ de e        ┆   ┆ (matemática ┆ principal  ┆      ┆     │\n",
       "│            ┆ Université  ┆ Terra       ┆ Estatística ┆   ┆ aplicada)…  ┆ objetivo   ┆      ┆     │\n",
       "│            ┆ d…          ┆             ┆             ┆   ┆             ┆ da visita… ┆      ┆     │\n",
       "└────────────┴─────────────┴─────────────┴─────────────┴───┴─────────────┴────────────┴──────┴─────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.path.join(PROJECT_ROOT, 'data', 'internal', 'fapesp_projects')\n",
    "\n",
    "full_data = extract(data_path)\n",
    "\n",
    "variables = {\n",
    "'N. Processo_B.V': 'n_processo',\n",
    "'Data de Início': 'data',\n",
    "'Título (Português)': 'titulo',\n",
    "'Grande Área do Conhecimento': 'grande_area',\n",
    "'Área do Conhecimento': 'area',\n",
    "'Subárea do Conhecimento': 'subarea',\n",
    "'Palavras-Chave do Processo': 'palavras_chave',\n",
    "'Assuntos': 'assuntos',\n",
    "'Resumo (Português)': 'resumo'}\n",
    "\n",
    "full_data = (\n",
    "    full_data\n",
    "    .lazy()\n",
    "    .rename(variables)\n",
    "    .select(variables.values())\n",
    "    .filter(\n",
    "        pl.col('n_processo').is_not_null(),\n",
    "        pl.col('resumo').is_not_null(),\n",
    "        pl.col('resumo') != '')\n",
    "    .with_columns(\n",
    "        pl.col('data').str.to_datetime('%m-%d-%y').dt.year().alias('ano'),\n",
    "        pl.col('data').str.to_datetime('%m-%d-%y').dt.month().alias('mes'))\n",
    "    .select(pl.exclude('data'))\n",
    ").collect()\n",
    "\n",
    "full_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234228, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_test = full_data.filter(pl.col('assuntos').is_not_null())\n",
    "\n",
    "data_train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, special_char_remover=re.compile(r'[^A-Za-z\\s]')):\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(\"O argumento 'text' deve ser uma string.\")\n",
    "    \n",
    "    text = special_char_remover.sub('', text)\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados\n",
    "data = data_train_test.to_pandas()\n",
    "\n",
    "data['titulo'] = data['titulo'].astype(str)\n",
    "data['palavras_chave'] = data['palavras_chave'].astype(str)\n",
    "\n",
    "data['cleaned_text'] = data['resumo'].apply(clean_text)\n",
    "data['cleaned_text'] += ' Título: ' + data['titulo'].apply(clean_text) + ' Palavras-chave: ' + data['palavras_chave'].apply(clean_text)\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenização com BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "train_tokenized_texts = tokenizer(train_data['cleaned_text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_tokenized_texts = tokenizer(test_data['cleaned_text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Converte os assuntos em listas\n",
    "train_data['assuntos_list'] = train_data['assuntos'].apply(lambda x: x.split(':'))\n",
    "test_data['assuntos_list'] = test_data['assuntos'].apply(lambda x: x.split(':'))\n",
    "\n",
    "# Binariza os rótulos\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "train_binary_labels = mlb.fit_transform(train_data['assuntos_list'])\n",
    "test_binary_labels = mlb.transform(test_data['assuntos_list'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo BERT pré-treinado\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(mlb.classes_))\n",
    "\n",
    "# Ajuste da camada de classificação para multi-rótulo\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(model.config.hidden_size, model.config.hidden_size),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(model.config.hidden_size, len(mlb.classes_)),\n",
    "    torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de perda e otimizador\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "# Scheduler de taxa de aprendizado\n",
    "total_steps = len(train_tokenized_texts['input_ids']) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# Criar datasets e dataloaders para treino e teste\n",
    "train_dataset = CustomDataset(train_tokenized_texts, train_binary_labels)\n",
    "test_dataset = CustomDataset(test_tokenized_texts, test_binary_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de avaliação\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return avg_loss, all_labels, all_preds\n",
    "\n",
    "# Função para calcular métricas\n",
    "\n",
    "def compute_metrics(labels, preds, threshold=0.5):\n",
    "    preds = (preds > threshold).astype(int)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='samples')\n",
    "    recall = recall_score(labels, preds, average='samples')\n",
    "    precision = precision_score(labels, preds, average='samples')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'recall': recall,\n",
    "        'precision': precision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop de Treinamento e Avaliação\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    \n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer, scheduler, device)\n",
    "    print(f'Training loss: {train_loss:.4f}')\n",
    "    \n",
    "    val_loss, val_labels, val_preds = evaluate(model, test_dataloader, criterion, device)\n",
    "    print(f'Validation loss: {val_loss:.4f}')\n",
    "    \n",
    "    metrics = compute_metrics(np.array(val_labels), np.array(val_preds))\n",
    "    print(f'Validation metrics: {metrics}')\n",
    "    \n",
    "    if metrics['f1'] > best_f1:\n",
    "        print(f'Saving best model with F1 score: {metrics[\"f1\"]:.4f}')\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        best_f1 = metrics['f1']\n",
    "\n",
    "# Carregar o modelo fine-tuned\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o modelo fine-tuned\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Função para gerar embeddings\n",
    "def get_embeddings(texts, model, tokenizer):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.bert(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token embeddings\n",
    "    return embeddings\n",
    "\n",
    "embeddings = get_embeddings(test_data['cleaned_text'].tolist(), model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar BERTopic com os embeddings ajustados\n",
    "topic_model = BERTopic(embedding_model=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# Ajustar o modelo aos dados\n",
    "topics, probabilities = topic_model.fit_transform(test_data['cleaned_text'].tolist(), embeddings.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar hiperparâmetros de UMAP e HDBSCAN conforme necessário\n",
    "# Por exemplo, ajustar o número de vizinhos em UMAP ou a densidade mínima em HDBSCAN\n",
    "\n",
    "# Visualizar os tópicos\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para predizer assuntos com o modelo BERT fine-tuned\n",
    "def predict_labels(text, model, tokenizer, threshold=0.5):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "    preds = (probs > threshold).astype(int)\n",
    "    return preds, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para aplicar BERTopic e identificar tópicos\n",
    "def apply_bertopic(text, topic_model):\n",
    "    topics, _ = topic_model.transform([text])\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(text, model, tokenizer, threshold=0.5):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "    preds = (probs > threshold).astype(int)\n",
    "    return preds, probs\n",
    "\n",
    "def apply_bertopic(text, topic_model, bert_model, tokenizer):\n",
    "    embeddings = get_embeddings([text], bert_model, tokenizer)\n",
    "    topics, _ = topic_model.transform(embeddings.numpy())\n",
    "    return topics\n",
    "\n",
    "def consolidate_results(text, model, tokenizer, topic_model, mlb, threshold=0.5):\n",
    "    preds, probs = predict_labels(text, model, tokenizer, threshold)\n",
    "    known_labels = mlb.inverse_transform(preds)\n",
    "    \n",
    "    # Se a confiança máxima estiver abaixo do threshold, use BERTopic\n",
    "    if np.max(probs) < threshold:\n",
    "        topics = apply_bertopic(text, topic_model, model, tokenizer)\n",
    "        topic_labels = [topic_model.get_topic(topic)[0] for topic in topics if topic != -1]\n",
    "        all_labels = list(set(known_labels[0]) | set(topic_labels))\n",
    "    else:\n",
    "        all_labels = known_labels[0]\n",
    "    \n",
    "    return all_labels\n",
    "\n",
    "# Exemplo de uso:\n",
    "text = \"Este projeto investiga o impacto da irrigação na produção de soja em diferentes tipos de solo.\"\n",
    "consolidated_labels = consolidate_results(text, model, tokenizer, topic_model, mlb, threshold=0.7)\n",
    "print(\"Assuntos Consolidados:\", consolidated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
