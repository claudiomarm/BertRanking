{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de bibliotecas\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from bertopic import BERTopic\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicao da raiz do projeto\n",
    "\n",
    "PROJECT_ROOT = 'G:/Csouza/nlp/topic_modeling'\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "sys.path.insert(0, PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(extract_path, file_name='all_process.xlsx', sheet_name='Sheet1'):\n",
    "    \n",
    "    return pl.read_excel(f'{extract_path}/{file_name}', sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>n_processo</th><th>titulo</th><th>grande_area</th><th>area</th><th>subarea</th><th>palavras_chave</th><th>assuntos</th><th>resumo</th><th>ano</th><th>mes</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i32</td><td>i8</td></tr></thead><tbody><tr><td>&quot;95/04916-0&quot;</td><td>&quot;Estudo sistemático de campos h…</td><td>&quot;Ciências Exatas e da Terra&quot;</td><td>&quot;Física&quot;</td><td>&quot;Física da Matéria Condensada&quot;</td><td>&quot;CORRELACAO ANGULAR, ESTUDO SIS…</td><td>null</td><td>&quot;Este projeto está vinculado ao…</td><td>1995</td><td>12</td></tr><tr><td>&quot;95/05064-7&quot;</td><td>&quot;Cultura, ideologia e represent…</td><td>&quot;Ciências Humanas&quot;</td><td>&quot;Sociologia&quot;</td><td>&quot;Outras Sociologias Específicas&quot;</td><td>&quot;BRASIL, IDENTIDADE, PENSAMENTO…</td><td>&quot;Brasil:Identidade social&quot;</td><td>&quot;Participar do Seminário &quot;&quot;Soci…</td><td>1995</td><td>12</td></tr><tr><td>&quot;95/09836-4&quot;</td><td>&quot;Bernard Schmitt | Université d…</td><td>&quot;Ciências Exatas e da Terra&quot;</td><td>&quot;Probabilidade e Estatística&quot;</td><td>&quot;Probabilidade&quot;</td><td>&quot;COMPRESSOR, ENTROPIA, ESTADO D…</td><td>&quot;Entropia (matemática aplicada)…</td><td>&quot;O principal objetivo da visita…</td><td>1995</td><td>12</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 10)\n",
       "┌────────────┬─────────────┬─────────────┬─────────────┬───┬─────────────┬────────────┬──────┬─────┐\n",
       "│ n_processo ┆ titulo      ┆ grande_area ┆ area        ┆ … ┆ assuntos    ┆ resumo     ┆ ano  ┆ mes │\n",
       "│ ---        ┆ ---         ┆ ---         ┆ ---         ┆   ┆ ---         ┆ ---        ┆ ---  ┆ --- │\n",
       "│ str        ┆ str         ┆ str         ┆ str         ┆   ┆ str         ┆ str        ┆ i32  ┆ i8  │\n",
       "╞════════════╪═════════════╪═════════════╪═════════════╪═══╪═════════════╪════════════╪══════╪═════╡\n",
       "│ 95/04916-0 ┆ Estudo      ┆ Ciências    ┆ Física      ┆ … ┆ null        ┆ Este       ┆ 1995 ┆ 12  │\n",
       "│            ┆ sistemático ┆ Exatas e da ┆             ┆   ┆             ┆ projeto    ┆      ┆     │\n",
       "│            ┆ de campos   ┆ Terra       ┆             ┆   ┆             ┆ está       ┆      ┆     │\n",
       "│            ┆ h…          ┆             ┆             ┆   ┆             ┆ vinculado  ┆      ┆     │\n",
       "│            ┆             ┆             ┆             ┆   ┆             ┆ ao…        ┆      ┆     │\n",
       "│ 95/05064-7 ┆ Cultura,    ┆ Ciências    ┆ Sociologia  ┆ … ┆ Brasil:Iden ┆ Participar ┆ 1995 ┆ 12  │\n",
       "│            ┆ ideologia e ┆ Humanas     ┆             ┆   ┆ tidade      ┆ do         ┆      ┆     │\n",
       "│            ┆ represent…  ┆             ┆             ┆   ┆ social      ┆ Seminário  ┆      ┆     │\n",
       "│            ┆             ┆             ┆             ┆   ┆             ┆ \"\"Soci…    ┆      ┆     │\n",
       "│ 95/09836-4 ┆ Bernard     ┆ Ciências    ┆ Probabilida ┆ … ┆ Entropia    ┆ O          ┆ 1995 ┆ 12  │\n",
       "│            ┆ Schmitt |   ┆ Exatas e da ┆ de e        ┆   ┆ (matemática ┆ principal  ┆      ┆     │\n",
       "│            ┆ Université  ┆ Terra       ┆ Estatística ┆   ┆ aplicada)…  ┆ objetivo   ┆      ┆     │\n",
       "│            ┆ d…          ┆             ┆             ┆   ┆             ┆ da visita… ┆      ┆     │\n",
       "└────────────┴─────────────┴─────────────┴─────────────┴───┴─────────────┴────────────┴──────┴─────┘"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.path.join(PROJECT_ROOT, 'data', 'internal', 'fapesp_projects')\n",
    "\n",
    "full_data = extract(data_path)\n",
    "\n",
    "variables = {\n",
    "'N. Processo_B.V': 'n_processo',\n",
    "'Data de Início': 'data',\n",
    "'Título (Português)': 'titulo',\n",
    "'Grande Área do Conhecimento': 'grande_area',\n",
    "'Área do Conhecimento': 'area',\n",
    "'Subárea do Conhecimento': 'subarea',\n",
    "'Palavras-Chave do Processo': 'palavras_chave',\n",
    "'Assuntos': 'assuntos',\n",
    "'Resumo (Português)': 'resumo'}\n",
    "\n",
    "full_data = (\n",
    "    full_data\n",
    "    .lazy()\n",
    "    .rename(variables)\n",
    "    .select(variables.values())\n",
    "    .filter(\n",
    "        pl.col('n_processo').is_not_null(),\n",
    "        pl.col('resumo').is_not_null(),\n",
    "        pl.col('resumo') != '')\n",
    "    .with_columns(\n",
    "        pl.col('data').str.to_datetime('%m-%d-%y').dt.year().alias('ano'),\n",
    "        pl.col('data').str.to_datetime('%m-%d-%y').dt.month().alias('mes'))\n",
    "    .select(pl.exclude('data'))\n",
    ").collect()\n",
    "\n",
    "full_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17342, 10)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_test = full_data.filter(pl.col('assuntos').is_not_null(), pl.col('area') == 'Medicina')\n",
    "\n",
    "data_train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_model(model='en_core_web_sm'):\n",
    "    \"\"\"\n",
    "    Baixa o modelo de linguagem spaCy se não estiver presente.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nlp = spacy.load(model)\n",
    "    except OSError:\n",
    "        from spacy.cli import download\n",
    "        download(model)\n",
    "        nlp = spacy.load(model)\n",
    "    return nlp\n",
    "\n",
    "# Carregar o modelo de linguagem em português do spaCy\n",
    "nlp = get_spacy_model('pt_core_news_sm')\n",
    "\n",
    "# Definir as stop words em português usando spaCy\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# Compilador para remover caracteres especiais (exceto acentos e espaços)\n",
    "special_char_remover = re.compile(r'[^A-Za-zÀ-ÿ\\s]')\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(\"O argumento 'text' deve ser uma string.\")\n",
    "    \n",
    "    # Remover caracteres especiais\n",
    "    text = special_char_remover.sub('', text)\n",
    "    \n",
    "    # Tokenizar o texto e remover stop words\n",
    "    tokens = [token.text for token in nlp(text) if token.text not in stop_words]\n",
    "    \n",
    "    # Lematizar o texto\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    text = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    return text\n",
    "\n",
    "# Carregar os dados\n",
    "data = data_train_test.to_pandas()\n",
    "\n",
    "data['titulo'] = data['titulo'].astype(str)\n",
    "data['palavras_chave'] = data['palavras_chave'].astype(str)\n",
    "\n",
    "data['cleaned_text'] = data['resumo'].apply(clean_text)\n",
    "data['cleaned_text'] += ' Título: ' + data['titulo'].apply(clean_text) + ' Palavras-chave: ' + data['palavras_chave'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir os dados em treino e teste\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenização com XLM-RoBERTa\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "train_tokenized_texts = tokenizer(train_data['cleaned_text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_tokenized_texts = tokenizer(test_data['cleaned_text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Converte os assuntos em listas\n",
    "train_data['assuntos_list'] = train_data['assuntos'].apply(lambda x: x.split(':'))\n",
    "test_data['assuntos_list'] = test_data['assuntos'].apply(lambda x: x.split(':'))\n",
    "\n",
    "# Binariza os rótulos\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "train_binary_labels = mlb.fit_transform(train_data['assuntos_list'])\n",
    "test_binary_labels = mlb.transform(test_data['assuntos_list'])\n",
    "\n",
    "# Convertendo para tensores PyTorch\n",
    "train_binary_labels = torch.tensor(train_binary_labels, dtype=torch.float)\n",
    "test_binary_labels = torch.tensor(test_binary_labels, dtype=torch.float)\n",
    "\n",
    "# Incluir as máscaras de atenção nos dados tokenizados\n",
    "train_attention_mask = train_tokenized_texts['attention_mask']\n",
    "test_attention_mask = test_tokenized_texts['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe Customizada para Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Criação dos DataLoaders\n",
    "train_dataset = CustomDataset(train_tokenized_texts, train_binary_labels)\n",
    "test_dataset = CustomDataset(test_tokenized_texts, test_binary_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Modelo XLM-RoBERTa pré-treinado\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=len(mlb.classes_))\n",
    "\n",
    "# Ajuste da camada de classificação para multi-rótulo\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(p=0.3),  # Adicionando Dropout\n",
    "    torch.nn.Linear(model.config.hidden_size, model.config.hidden_size),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.3),  # Adicionando Dropout\n",
    "    torch.nn.Linear(model.config.hidden_size, len(mlb.classes_)),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Congelar todas as camadas do BERT\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Descongelar as últimas 4 camadas\n",
    "for layer in model.roberta.encoder.layer[-4:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# A camada de classificação é treinada por padrão\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Obter os logits de classificação\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits[:, 0, :]  # Pegue apenas os logits do token [CLS]\n",
    "\n",
    "        # Calcular a função de perda\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Obter os logits de classificação\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits[:, 0, :]  # Pegue apenas os logits do token [CLS]\n",
    "\n",
    "            # Calcular a função de perda\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(logits.cpu().numpy())\n",
    "\n",
    "    return total_loss / len(dataloader), np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "def compute_metrics(labels, preds, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calcula métricas de avaliação para classificação de múltiplos rótulos.\n",
    "\n",
    "    Args:\n",
    "    labels (np.ndarray): Rótulos verdadeiros binarizados.\n",
    "    preds (np.ndarray): Predições do modelo (logits).\n",
    "    threshold (float): Limite para converter logits em valores binários.\n",
    "\n",
    "    Returns:\n",
    "    dict: Um dicionário contendo as métricas de avaliação.\n",
    "    \"\"\"\n",
    "    # Converter logits em predições binárias usando o threshold\n",
    "    preds = (preds > threshold).astype(int)\n",
    "    \n",
    "    # Calcular a acurácia\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    # Calcular o F1-score (média por amostra)\n",
    "    f1 = f1_score(labels, preds, average='samples')\n",
    "    \n",
    "    # Calcular o recall (média por amostra)\n",
    "    recall = recall_score(labels, preds, average='samples')\n",
    "    \n",
    "    # Calcular a precisão (média por amostra)\n",
    "    precision = precision_score(labels, preds, average='samples')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'recall': recall,\n",
    "        'precision': precision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(PROJECT_ROOT, 'models')\n",
    "bert_name = f'{model_path}/roberta_model.pt'\n",
    "\n",
    "# Configuração de hiperparâmetros\n",
    "learning_rate = 1e-5  # Pode ser ajustado para 2e-5 ou 5e-6 conforme necessário\n",
    "batch_size = 16\n",
    "num_epochs = 5  # Aumentar o número de épocas para garantir um melhor treinamento\n",
    "eps = 1e-8\n",
    "weight_decay = 0.01\n",
    "warmup_steps = 0  # Pode ser ajustado dependendo do dataset\n",
    "\n",
    "# Função de perda e otimizador\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=eps, weight_decay=weight_decay)\n",
    "\n",
    "# Scheduler de taxa de aprendizado\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# Verificar se o modelo já existe\n",
    "if not os.path.exists(bert_name):\n",
    "    # Loop de Treinamento e Avaliação\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    best_f1 = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        \n",
    "        # Treinamento\n",
    "        train_loss = train(model, train_dataloader, criterion, optimizer, scheduler, device)\n",
    "        print(f'Training loss: {train_loss:.4f}')\n",
    "        \n",
    "        # Avaliação\n",
    "        val_loss, val_labels, val_preds = evaluate(model, test_dataloader, criterion, device)\n",
    "        print(f'Validation loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Calcular métricas\n",
    "        metrics = compute_metrics(np.array(val_labels), np.array(val_preds))\n",
    "        print(f'Validation metrics: {metrics}')\n",
    "        \n",
    "        # Salvar o melhor modelo com base na métrica F1\n",
    "        if metrics['f1'] > best_f1:\n",
    "            print(f'Saving best model with F1 score: {metrics[\"f1\"]:.4f}')\n",
    "            torch.save(model.state_dict(), bert_name)\n",
    "            best_f1 = metrics['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o modelo fine-tuned\n",
    "model.load_state_dict(torch.load(bert_name))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_in_batches(texts, model, tokenizer, batch_size=8):\n",
    "    all_embeddings = []\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # Último estado oculto\n",
    "        cls_embeddings = hidden_states[:, 0, :]  # Pegue os embeddings do token [CLS]\n",
    "        all_embeddings.append(cls_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "def save_embeddings(embeddings, embeddings_path):\n",
    "    \"\"\"\n",
    "    Salva os embeddings em um arquivo .npy.\n",
    "\n",
    "    Args:\n",
    "    embeddings (numpy.ndarray): Embeddings a serem salvos.\n",
    "    embeddings_path (str): Caminho para salvar o arquivo .npy.\n",
    "    \"\"\"\n",
    "    np.save(embeddings_path, embeddings)\n",
    "    print(f'Embeddings salvos em {embeddings_path}')\n",
    "\n",
    "def load_embeddings(embeddings_path):\n",
    "    \"\"\"\n",
    "    Carrega os embeddings de um arquivo .npy.\n",
    "\n",
    "    Args:\n",
    "    embeddings_path (str): Caminho para o arquivo .npy contendo os embeddings.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Os embeddings carregados do arquivo.\n",
    "    \"\"\"\n",
    "    if os.path.exists(embeddings_path):\n",
    "        print(f'Loading embeddings from {embeddings_path}')\n",
    "        return np.load(embeddings_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f'Embeddings file not found at {embeddings_path}')\n",
    "\n",
    "def train_bertopic(docs, clean_text=None, use_embeddings=False, precomputed_embeddings=None, bert_model=None, tokenizer=None, umap_n_neighbors=15, umap_n_components=5, umap_min_dist=0.0, umap_metric='cosine', vectorizer_ngram_range=(1, 2), stop_words=stop_words):\n",
    "    # Pré-processar os documentos\n",
    "    if clean_text:\n",
    "        docs = [clean_text(doc) for doc in docs]\n",
    "    \n",
    "    if not isinstance(stop_words, list):\n",
    "        stop_words = list(stop_words)\n",
    "    \n",
    "    # Configurações padrão do BERTopic\n",
    "    umap_model = UMAP(n_neighbors=umap_n_neighbors, n_components=umap_n_components, min_dist=umap_min_dist, metric=umap_metric)\n",
    "    vectorizer_model = CountVectorizer(ngram_range=vectorizer_ngram_range, stop_words=stop_words)\n",
    "    \n",
    "    # Check if we should use embeddings\n",
    "    if use_embeddings:\n",
    "        embeddings = precomputed_embeddings\n",
    "        if embeddings is None:\n",
    "            assert bert_model is not None, \"O modelo deve ser fornecido quando use_embeddings=True\"\n",
    "            assert tokenizer is not None, \"O tokenizador deve ser fornecido quando use_embeddings=True\"\n",
    "            \n",
    "            # Extrair embeddings para os documentos em lotes\n",
    "            embeddings = extract_embeddings_in_batches(docs, bert_model, tokenizer)\n",
    "            \n",
    "            # Criar e treinar o modelo BERTopic usando os embeddings\n",
    "            topic_model = BERTopic(umap_model=umap_model, vectorizer_model=vectorizer_model)\n",
    "            topic_model.fit(docs, embeddings)\n",
    "    else:\n",
    "        # Criar e treinar o modelo BERTopic usando a configuração padrão\n",
    "        topic_model = BERTopic(umap_model=umap_model, vectorizer_model=vectorizer_model)\n",
    "        topic_model.fit(docs)\n",
    "    \n",
    "    return topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertopic_name = f'{model_path}/bertopic_model.pt'\n",
    "\n",
    "if not os.path.exists(bertopic_name):\n",
    "    text_train = train_data['cleaned_text'].tolist()\n",
    "    bertopic_model = train_bertopic(docs=text_train)\n",
    "    bertopic_model.save(bertopic_name)\n",
    "\n",
    "bertopic_model = BERTopic.load(bertopic_name)\n",
    "bertopic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_name = f'{model_path}/roberta_embeddings.npy'\n",
    "if not os.path.exists(embeddings_name):\n",
    "    text_embeddings = train_data['cleaned_text'].tolist()\n",
    "    embeddings = extract_embeddings_in_batches(text_embeddings, model, tokenizer)\n",
    "    save_embeddings(embeddings, embeddings_name)\n",
    "\n",
    "embeddings = load_embeddings(embeddings_name)\n",
    "\n",
    "bertopic_roberta_embeddings_name = f'{model_path}/bertopic_roberta_embeddings_model.pt'\n",
    "if not os.path.exists(bertopic_roberta_embeddings_name):\n",
    "    text_train = train_data['cleaned_text'].tolist()\n",
    "    bertopic_roberta_embeddings_model = train_bertopic(docs=text_train, use_embeddings=True, precomputed_embeddings=embeddings)\n",
    "    bertopic_roberta_embeddings_model.save(bertopic_roberta_embeddings_name)\n",
    "\n",
    "bertopic_roberta_embeddings_model = BERTopic.load(bertopic_roberta_embeddings_name)\n",
    "bertopic_roberta_embeddings_model.visualize_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
