{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de bibliotecas\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import logging\n",
    "import logging.config\n",
    "import requests\n",
    "import fitz\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_openalex(entity='works', **kwargs):\n",
    "    \"\"\"\n",
    "    Função genérica para buscar na OpenAlex.\n",
    "\n",
    "    Args:\n",
    "    - entity (str): Tipo de entidade a ser buscada ('works' ou 'concepts').\n",
    "    - **kwargs: Parâmetros de consulta para a API da OpenAlex.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Resposta da API da OpenAlex em formato JSON.\n",
    "    \"\"\"\n",
    "    base_url = f'https://api.openalex.org/{entity}'\n",
    "    try:\n",
    "        response = requests.get(base_url, params=kwargs)\n",
    "\n",
    "        return response.json()\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f'Request failed: {str(e)}')\n",
    "        raise e\n",
    "\n",
    "def get_pdf_url(work):\n",
    "    \"\"\"\n",
    "    Verifica e retorna o URL de PDF de acesso aberto para um trabalho.\n",
    "\n",
    "    Args:\n",
    "    - work (dict): Metadados de um trabalho.\n",
    "\n",
    "    Returns:\n",
    "    - str: URL de PDF de acesso aberto ou None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'primary_location' in work and work['primary_location']:\n",
    "            pdf_url = work['primary_location'].get('pdf_url')\n",
    "\n",
    "            if pdf_url:\n",
    "                str_pdf_url = pdf_url.split('/')[-1]\n",
    "                str_pdf_url = str_pdf_url.split('.')[-1]\n",
    "\n",
    "                if str_pdf_url == 'pdf':\n",
    "                    return pdf_url\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f'Error: {str(e)}')\n",
    "        raise e\n",
    "\n",
    "def extract_text_from_pdf_url(pdf_url):\n",
    "    \"\"\"\n",
    "    Extrai o texto de um PDF diretamente de uma URL.\n",
    "\n",
    "    Args:\n",
    "    - pdf_url (str): URL do PDF.\n",
    "\n",
    "    Returns:\n",
    "    - str: Texto extraído do PDF.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        response = requests.get(pdf_url)\n",
    "        response.raise_for_status()\n",
    "        pdf_bytes = response.content\n",
    "        with fitz.open(stream=pdf_bytes, filetype=\"pdf\") as doc:\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc.load_page(page_num)\n",
    "                text += page.get_text()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Limpa o texto extraído de um PDF e extrai apenas o abstract.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Texto extraído do PDF.\n",
    "\n",
    "    Returns:\n",
    "    - str: Texto limpo contendo apenas o abstract.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Substituir múltiplas quebras de linha e espaços por um único espaço\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        # Remover caracteres de controle como \\xa0 e \\u2003\n",
    "        text = text.replace(u'\\xa0', u' ').replace(u'\\u2003', u' ')\n",
    "\n",
    "        # Remover URLs (opcional)\n",
    "        text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "\n",
    "        # Extrair apenas o abstract\n",
    "        abstract_start = re.search(r'\\bAbstract\\b', text, re.IGNORECASE)\n",
    "        if abstract_start:\n",
    "            # Encontrar o início do abstract\n",
    "            start_pos = abstract_start.end()\n",
    "            # Encontrar o próximo parágrafo em branco\n",
    "            end_pos = re.search(r'\\n\\s*\\n', text[start_pos:])\n",
    "            if end_pos:\n",
    "                end_pos = start_pos + end_pos.start()\n",
    "                text = text[start_pos:end_pos].strip()\n",
    "            else:\n",
    "                text = text[start_pos:].strip()\n",
    "        else:\n",
    "            # Se não encontrar \"Abstract\", retorna o texto original limpo\n",
    "            text = text.strip()\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error: {str(e)}')\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Pré-processa o texto para uso em um modelo de PLN.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Texto a ser pré-processado.\n",
    "\n",
    "    Returns:\n",
    "    - str: Texto pré-processado.\n",
    "    \"\"\"\n",
    "    # Remover caracteres especiais, números e sinais de pontuação\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Processar o texto com spaCy\n",
    "    doc = nlp_model(text)\n",
    "\n",
    "    # Tokenizar, remover stopwords e lematizar\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "\n",
    "    # Reunir tokens em uma string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "def download_spacy_model():\n",
    "    \"\"\"\n",
    "    Baixa o modelo de linguagem spaCy se não estiver presente.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "    except OSError:\n",
    "        from spacy.cli import download\n",
    "        download('en_core_web_sm')\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "    return nlp\n",
    "\n",
    "def get_representative_words(text, tokenizer, model, tfidf_scores, top_n=5):\n",
    "    \"\"\"\n",
    "    Gera embeddings contextuais para o texto e extrai as palavras mais representativas.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Texto a ser processado.\n",
    "    - tfidf_scores: Pontuações TF-IDF para o texto.\n",
    "    - top_n (int): Número de palavras representativas a serem extraídas.\n",
    "\n",
    "    Returns:\n",
    "    - list: Lista de palavras mais representativas.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    word_embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "    similarities = cosine_similarity(word_embeddings, cls_embedding, dim=-1)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze(0))\n",
    "\n",
    "    word_similarities = list(zip(tokens, similarities.tolist(), tfidf_scores.tolist()))\n",
    "    word_similarities.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "\n",
    "    representative_words = [word for word, sim, tfidf in word_similarities if word not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"] and len(word) > 3][:top_n]\n",
    "\n",
    "    return representative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Environmental health': 'C99454951',\n",
       " 'Health care': 'C160735492',\n",
       " 'Public health': 'C138816342',\n",
       " 'Mental health': 'C134362201',\n",
       " 'Occupational safety and health': 'C187155963',\n",
       " 'Human health': 'C2987857752',\n",
       " 'Health equity': 'C2250968',\n",
       " 'Health psychology': 'C155164915',\n",
       " 'Global health': 'C46578552',\n",
       " 'Health policy': 'C47344431',\n",
       " 'Health promotion': 'C185618831',\n",
       " 'Health informatics': 'C145642194',\n",
       " 'Oral health': 'C2992672162',\n",
       " 'Reproductive health': 'C121752807',\n",
       " 'Health services': 'C2986740045',\n",
       " 'Health professionals': 'C3019806175',\n",
       " 'Health benefits': 'C3018122547',\n",
       " 'Health administration': 'C137992405',\n",
       " 'Health education': 'C113807197',\n",
       " 'National Health and Nutrition Examination Survey': 'C2779874844',\n",
       " 'Health insurance': 'C2983635472',\n",
       " 'Population health': 'C2778149918',\n",
       " 'Social determinants of health': 'C78491826',\n",
       " 'Health literacy': 'C2778843546',\n",
       " 'Community health': 'C2775951005'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Buscar conceitos relacionados à saúde\n",
    "concept_params = {\n",
    "    'search': 'health'\n",
    "}\n",
    "concept_result = search_openalex(entity='concepts', **concept_params)\n",
    "\n",
    "concepts = {}\n",
    "for concept in concept_result['results']:\n",
    "    concepts[concept['display_name']] = concept['id'].split('/')[-1]\n",
    "\n",
    "concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_id = concepts['Health psychology']\n",
    "\n",
    "# Definir parâmetros de paginação\n",
    "per_page = 40\n",
    "\n",
    "# Buscar artigos em inglês de 2022 a 2024 dos conceitos selecionados, com paginação\n",
    "work_params = {\n",
    "    'filter': f'language:en,from_publication_date:2022-01-01,to_publication_date:2024-12-31,concepts.id:{concept_id}',\n",
    "    'per_page': per_page,\n",
    "}\n",
    "\n",
    "work_result = search_openalex(entity='works', **work_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error: 403 Client Error: Forbidden for url: https://academic.oup.com/abm/advance-article-pdf/doi/10.1093/abm/kaac039/45037045/kaac039.pdf\n",
      "ERROR:root:Error: 403 Client Error: Forbidden for url: https://academic.oup.com/tbm/advance-article-pdf/doi/10.1093/tbm/ibad014/50031314/ibad014.pdf\n",
      "ERROR:root:Error: 403 Client Error: Forbidden for url: https://academic.oup.com/abm/article-pdf/56/8/781/45214298/kaac023.pdf\n"
     ]
    }
   ],
   "source": [
    "texts = {}\n",
    "pdfs = {}\n",
    "for work in work_result['results']:\n",
    "    open_alex_id = work.get('id').split('/')[-1]\n",
    "    pdf_url = get_pdf_url(work)\n",
    "\n",
    "    text = False\n",
    "    if pdf_url:\n",
    "        text = extract_text_from_pdf_url(pdf_url)\n",
    "        text = clean_text(text)\n",
    "    \n",
    "    if open_alex_id and text:\n",
    "        texts[open_alex_id] = text\n",
    "        pdfs[open_alex_id] = pdf_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = download_spacy_model()\n",
    "\n",
    "prep_texts = {}\n",
    "for article_id, text in texts.items():\n",
    "    prep_texts[article_id] = preprocess_text(text, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W4207080146': ['research', 'lack', 'comprise', 'govern', 'solution'],\n",
       " 'W4206998862': ['administer', 'include', 'evenly', 'speak', 'survey'],\n",
       " 'W4205630067': ['validation', 'model', 'reliability', 'examine', 'extant'],\n",
       " 'W4226453222': ['analysis', 'research', 'examine', 'prevalence', 'complete'],\n",
       " 'W4210338148': ['social', 'initiative', 'research', 'suggest', 'develop'],\n",
       " 'W4220657291': ['evidence', '##le', 'propose', 'report', 'evidence'],\n",
       " 'W4210916174': ['regression', 'examine', 'model', 'association', 'examine'],\n",
       " 'W4225113282': ['widespread', 'percent', 'remain', 'perceive', 'compare'],\n",
       " 'W4213426836': ['include', 'access', 'proportion', 'include', 'include'],\n",
       " 'W4313424320': ['follow', '##ml', 'support', 'compare', 'examine'],\n",
       " 'W4315491255': ['individual', 'model', 'analysis', 'hypothetical', 'sample'],\n",
       " 'W4321434852': ['include', 'organize', 'address', 'particularly', 'develop'],\n",
       " 'W4321850362': ['change',\n",
       "  'prevention',\n",
       "  'approach',\n",
       "  'currently',\n",
       "  'participant'],\n",
       " 'W4361302816': ['previous', 'research', 'consist', 'examine', 'depression'],\n",
       " 'W4319454655': ['follow', 'availability', 'include', 'eligible', 'effect'],\n",
       " 'W4392550813': ['mechanism', 'remain', 'depression', 'object', 'involve'],\n",
       " 'W4205776545': ['people', 'analysis', 'analyze', 'sample', 'describe'],\n",
       " 'W4206670997': ['people', 'group', 'research', 'percent', 'engage'],\n",
       " 'W4211041831': ['restriction', 'include', 'year', 'publication', 'follow'],\n",
       " 'W4289522482': ['include', 'representation', 'include', 'identify', 'vary'],\n",
       " 'W4225537095': ['attempt', 'consider', 'controller', 'study', 'analysis']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = list(prep_texts.values())\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "representative_words_dict = {}\n",
    "\n",
    "for i, (article_id, text) in enumerate(prep_texts.items()):\n",
    "    tfidf_scores = tfidf_matrix[i].toarray().flatten()\n",
    "    representative_words = get_representative_words(text, tokenizer, model, tfidf_scores, top_n=5)\n",
    "    representative_words_dict[article_id] = representative_words\n",
    "\n",
    "representative_words_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
