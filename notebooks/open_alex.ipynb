{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de bibliotecas\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import logging\n",
    "import logging.config\n",
    "import requests\n",
    "import fitz\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xml.etree import ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_openalex(entity='works', **kwargs):\n",
    "    \"\"\"\n",
    "    Função genérica para buscar na OpenAlex.\n",
    "\n",
    "    Args:\n",
    "    - entity (str): Tipo de entidade a ser buscada ('works' ou 'concepts').\n",
    "    - **kwargs: Parâmetros de consulta para a API da OpenAlex.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Resposta da API da OpenAlex em formato JSON.\n",
    "    \"\"\"\n",
    "    base_url = f'https://api.openalex.org/{entity}'\n",
    "    try:\n",
    "        response = requests.get(base_url, params=kwargs)\n",
    "\n",
    "        return response.json()\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f'Request failed: {str(e)}')\n",
    "        raise e\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Pré-processa o texto para uso em um modelo de PLN.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Texto a ser pré-processado.\n",
    "\n",
    "    Returns:\n",
    "    - str: Texto pré-processado.\n",
    "    \"\"\"\n",
    "    # Remover caracteres especiais, números e sinais de pontuação\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "\n",
    "    # Converter para minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Processar o texto com spaCy\n",
    "    doc = nlp_model(text)\n",
    "\n",
    "    # Tokenizar, remover stopwords e lematizar\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "\n",
    "    # Reunir tokens em uma string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "def get_spacy_model():\n",
    "    \"\"\"\n",
    "    Baixa o modelo de linguagem spaCy se não estiver presente.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "    except OSError:\n",
    "        from spacy.cli import download\n",
    "        download('en_core_web_sm')\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "    return nlp\n",
    "\n",
    "def get_representative_words(text, tokenizer, model, tfidf_scores, top_n=5):\n",
    "    \"\"\"\n",
    "    Gera embeddings contextuais para o texto e extrai as palavras mais representativas.\n",
    "\n",
    "    Args:\n",
    "    - text (str): Texto a ser processado.\n",
    "    - tfidf_scores: Pontuações TF-IDF para o texto.\n",
    "    - top_n (int): Número de palavras representativas a serem extraídas.\n",
    "\n",
    "    Returns:\n",
    "    - list: Lista de palavras mais representativas.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    word_embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "    similarities = cosine_similarity(word_embeddings, cls_embedding, dim=-1)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze(0))\n",
    "\n",
    "    word_similarities = list(zip(tokens, similarities.tolist(), tfidf_scores.tolist()))\n",
    "    word_similarities.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "\n",
    "    representative_words = [word for word, sim, tfidf in word_similarities if word not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"] and len(word) > 3][:top_n]\n",
    "\n",
    "    return representative_words\n",
    "\n",
    "def search_pubmed(doi):\n",
    "    url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'\n",
    "    params = {\n",
    "        'db': 'pubmed',\n",
    "        'term': doi,\n",
    "        'retmode': 'json'\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    idlist = data['esearchresult']['idlist']\n",
    "    if idlist:\n",
    "        return idlist[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Função para recuperar o resumo completo do artigo usando o PMID\n",
    "def fetch_abstract(pmid):\n",
    "    url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "    params = {\n",
    "        'db': 'pubmed',\n",
    "        'id': pmid,\n",
    "        'retmode': 'xml'\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    root = ET.fromstring(response.content)\n",
    "    abstract_text = ''\n",
    "    for abstract in root.findall(\".//AbstractText\"):\n",
    "        abstract_text += abstract.text + ' '\n",
    "    return abstract_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar conceitos relacionados à saúde\n",
    "concept_params = {\n",
    "    'search': 'health'\n",
    "}\n",
    "concept_result = search_openalex(entity='concepts', **concept_params)\n",
    "\n",
    "concepts = {}\n",
    "for concept in concept_result['results']:\n",
    "    concepts[concept['display_name']] = concept['id'].split('/')[-1]\n",
    "\n",
    "concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_id = concepts['Health psychology']\n",
    "\n",
    "# Definir parâmetros de paginação\n",
    "per_page = 200\n",
    "\n",
    "# Buscar artigos em inglês de 2022 a 2024 dos conceitos selecionados, com paginação\n",
    "work_params = {\n",
    "    'filter': f'language:en,from_publication_date:2022-01-01,to_publication_date:2024-12-31,concepts.id:{concept_id}',\n",
    "    'per_page': per_page,\n",
    "}\n",
    "\n",
    "work_result = search_openalex(entity='works', **work_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_dict = {}\n",
    "for work in work_result['results']:\n",
    "    open_alex_id = work.get('id').split('/')[-1]\n",
    "    doi = work.get('doi')\n",
    "    doi = doi.split('org/')[-1]\n",
    "    \n",
    "    if open_alex_id and doi:\n",
    "        doi_dict[open_alex_id] = doi\n",
    "\n",
    "doi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = {}\n",
    "for article_id, doi in doi_dict.items():\n",
    "    pmid = search_pubmed(doi)\n",
    "    \n",
    "    if pmid:\n",
    "        abstract = fetch_abstract(pmid)\n",
    "        articles[article_id] = abstract\n",
    "    else:\n",
    "        print(f'Article of ID {article_id} not found in PubMed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = get_spacy_model()\n",
    "\n",
    "prep_texts = {}\n",
    "for article_id, text in articles.items():\n",
    "    prep_texts[article_id] = preprocess_text(text, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(prep_texts.values())\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "representative_words_dict = {}\n",
    "\n",
    "for i, (article_id, text) in enumerate(prep_texts.items()):\n",
    "    tfidf_scores = tfidf_matrix[i].toarray().flatten()\n",
    "    representative_words = get_representative_words(text, tokenizer, model, tfidf_scores, top_n=5)\n",
    "    representative_words_dict[article_id] = representative_words\n",
    "\n",
    "representative_words_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
