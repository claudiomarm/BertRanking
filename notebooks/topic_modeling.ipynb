{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\csouza\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\academic-fingerprint-4AictD0y-py3.11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importação de bibliotecas\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from bertopic import BERTopic\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicao da raiz do projeto\n",
    "\n",
    "PROJECT_ROOT = 'G:/Csouza/nlp/topic_modeling'\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "sys.path.insert(0, PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(extract_path, file_name='all_process.xlsx', sheet_name='Sheet1'):\n",
    "    \n",
    "    return pl.read_excel(f'{extract_path}/{file_name}', sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>n_processo</th><th>titulo</th><th>grande_area</th><th>area</th><th>subarea</th><th>palavras_chave</th><th>assuntos</th><th>resumo</th><th>ano</th><th>mes</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i32</td><td>i8</td></tr></thead><tbody><tr><td>&quot;95/04916-0&quot;</td><td>&quot;Estudo sistemático de campos h…</td><td>&quot;Ciências Exatas e da Terra&quot;</td><td>&quot;Física&quot;</td><td>&quot;Física da Matéria Condensada&quot;</td><td>&quot;CORRELACAO ANGULAR, ESTUDO SIS…</td><td>null</td><td>&quot;Este projeto está vinculado ao…</td><td>1995</td><td>12</td></tr><tr><td>&quot;95/05064-7&quot;</td><td>&quot;Cultura, ideologia e represent…</td><td>&quot;Ciências Humanas&quot;</td><td>&quot;Sociologia&quot;</td><td>&quot;Outras Sociologias Específicas&quot;</td><td>&quot;BRASIL, IDENTIDADE, PENSAMENTO…</td><td>&quot;Brasil:Identidade social&quot;</td><td>&quot;Participar do Seminário &quot;&quot;Soci…</td><td>1995</td><td>12</td></tr><tr><td>&quot;95/09836-4&quot;</td><td>&quot;Bernard Schmitt | Université d…</td><td>&quot;Ciências Exatas e da Terra&quot;</td><td>&quot;Probabilidade e Estatística&quot;</td><td>&quot;Probabilidade&quot;</td><td>&quot;COMPRESSOR, ENTROPIA, ESTADO D…</td><td>&quot;Entropia (matemática aplicada)…</td><td>&quot;O principal objetivo da visita…</td><td>1995</td><td>12</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 10)\n",
       "┌────────────┬─────────────┬─────────────┬─────────────┬───┬─────────────┬────────────┬──────┬─────┐\n",
       "│ n_processo ┆ titulo      ┆ grande_area ┆ area        ┆ … ┆ assuntos    ┆ resumo     ┆ ano  ┆ mes │\n",
       "│ ---        ┆ ---         ┆ ---         ┆ ---         ┆   ┆ ---         ┆ ---        ┆ ---  ┆ --- │\n",
       "│ str        ┆ str         ┆ str         ┆ str         ┆   ┆ str         ┆ str        ┆ i32  ┆ i8  │\n",
       "╞════════════╪═════════════╪═════════════╪═════════════╪═══╪═════════════╪════════════╪══════╪═════╡\n",
       "│ 95/04916-0 ┆ Estudo      ┆ Ciências    ┆ Física      ┆ … ┆ null        ┆ Este       ┆ 1995 ┆ 12  │\n",
       "│            ┆ sistemático ┆ Exatas e da ┆             ┆   ┆             ┆ projeto    ┆      ┆     │\n",
       "│            ┆ de campos   ┆ Terra       ┆             ┆   ┆             ┆ está       ┆      ┆     │\n",
       "│            ┆ h…          ┆             ┆             ┆   ┆             ┆ vinculado  ┆      ┆     │\n",
       "│            ┆             ┆             ┆             ┆   ┆             ┆ ao…        ┆      ┆     │\n",
       "│ 95/05064-7 ┆ Cultura,    ┆ Ciências    ┆ Sociologia  ┆ … ┆ Brasil:Iden ┆ Participar ┆ 1995 ┆ 12  │\n",
       "│            ┆ ideologia e ┆ Humanas     ┆             ┆   ┆ tidade      ┆ do         ┆      ┆     │\n",
       "│            ┆ represent…  ┆             ┆             ┆   ┆ social      ┆ Seminário  ┆      ┆     │\n",
       "│            ┆             ┆             ┆             ┆   ┆             ┆ \"\"Soci…    ┆      ┆     │\n",
       "│ 95/09836-4 ┆ Bernard     ┆ Ciências    ┆ Probabilida ┆ … ┆ Entropia    ┆ O          ┆ 1995 ┆ 12  │\n",
       "│            ┆ Schmitt |   ┆ Exatas e da ┆ de e        ┆   ┆ (matemática ┆ principal  ┆      ┆     │\n",
       "│            ┆ Université  ┆ Terra       ┆ Estatística ┆   ┆ aplicada)…  ┆ objetivo   ┆      ┆     │\n",
       "│            ┆ d…          ┆             ┆             ┆   ┆             ┆ da visita… ┆      ┆     │\n",
       "└────────────┴─────────────┴─────────────┴─────────────┴───┴─────────────┴────────────┴──────┴─────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.path.join(PROJECT_ROOT, 'data', 'internal', 'fapesp_projects')\n",
    "\n",
    "full_data = extract(data_path)\n",
    "\n",
    "variables = {\n",
    "'N. Processo_B.V': 'n_processo',\n",
    "'Data de Início': 'data',\n",
    "'Título (Português)': 'titulo',\n",
    "'Grande Área do Conhecimento': 'grande_area',\n",
    "'Área do Conhecimento': 'area',\n",
    "'Subárea do Conhecimento': 'subarea',\n",
    "'Palavras-Chave do Processo': 'palavras_chave',\n",
    "'Assuntos': 'assuntos',\n",
    "'Resumo (Português)': 'resumo'}\n",
    "\n",
    "full_data = (\n",
    "    full_data\n",
    "    .lazy()\n",
    "    .rename(variables)\n",
    "    .select(variables.values())\n",
    "    .filter(\n",
    "        pl.col('n_processo').is_not_null(),\n",
    "        pl.col('resumo').is_not_null(),\n",
    "        pl.col('resumo') != '')\n",
    "    .with_columns(\n",
    "        pl.col('data').str.to_datetime('%m-%d-%y').dt.year().alias('ano'),\n",
    "        pl.col('data').str.to_datetime('%m-%d-%y').dt.month().alias('mes'))\n",
    "    .select(pl.exclude('data'))\n",
    ").collect()\n",
    "\n",
    "full_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17342, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_test = full_data.filter(pl.col('assuntos').is_not_null(), pl.col('area') == 'Medicina')\n",
    "\n",
    "data_train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_model(model='en_core_web_sm'):\n",
    "    \"\"\"\n",
    "    Baixa o modelo de linguagem spaCy se não estiver presente.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nlp = spacy.load(model)\n",
    "    except OSError:\n",
    "        from spacy.cli import download\n",
    "        download(model)\n",
    "        nlp = spacy.load(model)\n",
    "    return nlp\n",
    "\n",
    "# Carregar o modelo de linguagem em português do spaCy\n",
    "nlp = get_spacy_model('pt_core_news_sm')\n",
    "\n",
    "# Definir as stop words em português usando spaCy\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# Compilador para remover caracteres especiais (exceto acentos e espaços)\n",
    "special_char_remover = re.compile(r'[^A-Za-zÀ-ÿ\\s]')\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(\"O argumento 'text' deve ser uma string.\")\n",
    "    \n",
    "    # Remover caracteres especiais\n",
    "    text = special_char_remover.sub('', text)\n",
    "    \n",
    "    # Tokenizar o texto e remover stop words\n",
    "    tokens = [token.text for token in nlp(text) if token.text not in stop_words]\n",
    "    \n",
    "    # Lematizar o texto\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    text = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    return text\n",
    "\n",
    "# Carregar os dados\n",
    "data = data_train_test.to_pandas()\n",
    "\n",
    "data['titulo'] = data['titulo'].astype(str)\n",
    "data['palavras_chave'] = data['palavras_chave'].astype(str)\n",
    "\n",
    "data['cleaned_text'] = data['resumo'].apply(clean_text)\n",
    "data['cleaned_text'] += ' Título: ' + data['titulo'].apply(clean_text) + ' Palavras-chave: ' + data['palavras_chave'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o tokenizer e o modelo pré-treinado\n",
    "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "bert_model = BertForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "\n",
    "# Tokenizar os dados\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['cleaned_text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "dataset = Dataset.from_pandas(data)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Dividir em conjunto de treino e teste\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, save_directory, model_name='bert_model.pt'):\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    model_path = os.path.join(save_directory, model_name)\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "def save_tokenizer(tokenizer, save_directory, tokenizer_name='tokenizer'):\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    tokenizer_path = os.path.join(save_directory, tokenizer_name)\n",
    "    \n",
    "    tokenizer.save_pretrained(tokenizer_path)\n",
    "\n",
    "def train_model(model, tokenizer, train_dataset, test_dataset, output_dir=None, num_train_epochs=3, per_device_train_batch_size=8, per_device_eval_batch_size=8, warmup_steps=500, weight_decay=0.01, logging_dir=None, logging_steps=10, model_save_dir='./models', model_name='bert_model.pt', tokenizer_save_dir='./models', tokenizer_name='tokenizer', save_logs=False, save_results=False):\n",
    "    \"\"\"\n",
    "    Treina o modelo BERT fine-tuned e salva o modelo e o tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model: O modelo BERT a ser treinado.\n",
    "        tokenizer: O tokenizer associado ao modelo.\n",
    "        train_dataset: O dataset de treinamento.\n",
    "        test_dataset: O dataset de validação.\n",
    "        output_dir: Diretório para salvar os resultados do treinamento. Se None, define um valor padrão.\n",
    "        num_train_epochs: Número de épocas de treinamento.\n",
    "        per_device_train_batch_size: Tamanho do batch de treinamento por dispositivo.\n",
    "        per_device_eval_batch_size: Tamanho do batch de avaliação por dispositivo.\n",
    "        warmup_steps: Número de passos de aquecimento.\n",
    "        weight_decay: Decaimento de peso (weight decay) para o otimizador.\n",
    "        logging_dir: Diretório para salvar os logs. Se None, define um valor padrão.\n",
    "        logging_steps: Passos de logging.\n",
    "        model_save_dir: Diretório para salvar o modelo fine-tuned.\n",
    "        model_name: Nome do arquivo do modelo.\n",
    "        tokenizer_save_dir: Diretório para salvar o tokenizer.\n",
    "        tokenizer_name: Nome do subdiretório onde o tokenizer será salvo.\n",
    "        save_logs: Booleano para decidir se os logs devem ser salvos.\n",
    "        save_results: Booleano para decidir se os resultados devem ser salvos.\n",
    "    \n",
    "    Returns:\n",
    "        trainer: O objeto Trainer após o treinamento.\n",
    "    \"\"\"\n",
    "    # Definir diretórios padrão se None for fornecido\n",
    "    if output_dir is None:\n",
    "        output_dir = './results'\n",
    "    if logging_dir is None:\n",
    "        logging_dir = './logs'\n",
    "    \n",
    "    # Criar os diretórios de saída se não existirem e se for necessário salvar\n",
    "    if save_results:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    if save_logs:\n",
    "        os.makedirs(logging_dir, exist_ok=True)\n",
    "\n",
    "    # Definir argumentos de treinamento\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir if save_results else None,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        warmup_steps=warmup_steps,\n",
    "        weight_decay=weight_decay,\n",
    "        logging_dir=logging_dir if save_logs else None,\n",
    "        logging_steps=logging_steps,\n",
    "    )\n",
    "\n",
    "    # Definir o Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    # Treinar o modelo\n",
    "    trainer.train()\n",
    "\n",
    "    # Salvar o modelo e o tokenizer\n",
    "    save_model(model, model_save_dir, model_name)\n",
    "    save_tokenizer(tokenizer, tokenizer_save_dir, tokenizer_name)\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "def evaluate_model(trainer, test_dataset, metric_average='weighted'):\n",
    "    \"\"\"\n",
    "    Avalia a qualidade do treinamento do modelo usando o conjunto de dados de teste.\n",
    "    \n",
    "    Args:\n",
    "        trainer: O objeto Trainer que foi usado para treinar o modelo.\n",
    "        test_dataset: O conjunto de dados de teste.\n",
    "        metric_average: Tipo de média a ser usada para calcular as métricas (ex: 'weighted', 'macro', 'micro').\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Um dicionário contendo as métricas calculadas.\n",
    "    \"\"\"\n",
    "    # Prever os rótulos do conjunto de teste\n",
    "    predictions, labels, _ = trainer.predict(test_dataset)\n",
    "    preds = predictions.argmax(axis=1)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average=metric_average)\n",
    "    recall = recall_score(labels, preds, average=metric_average)\n",
    "    f1 = f1_score(labels, preds, average=metric_average)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def load_model(save_directory, model_name='bert_model.pt', pretrained_model_name='neuralmind/bert-base-portuguese-cased', model_class=BertForSequenceClassification):\n",
    "    model_path = os.path.join(save_directory, model_name)\n",
    "    \n",
    "    # Carregar o modelo pré-treinado\n",
    "    model = model_class.from_pretrained(pretrained_model_name)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    print(f\"Modelo carregado de {model_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_tokenizer(save_directory, tokenizer_name='tokenizer'):\n",
    "    tokenizer_directory = os.path.join(save_directory, tokenizer_name)\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(tokenizer_directory)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def extract_embeddings(texts, model, tokenizer, batch_size=8):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        all_embeddings.append(batch_embeddings.cpu())\n",
    "    \n",
    "    # Concatenar todos os embeddings em um único tensor\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    return all_embeddings\n",
    "\n",
    "def save_embeddings(embeddings, embeddings_path):\n",
    "    np.save(embeddings_path, embeddings)\n",
    "    \n",
    "def load_embeddings(embeddings_path):\n",
    "    if os.path.exists(embeddings_path):\n",
    "        return np.load(embeddings_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f'Embeddings file not found at {embeddings_path}')\n",
    "\n",
    "def train_bertopic(docs, clean_text=None, use_embeddings=False, precomputed_embeddings=None, bert_model=None, tokenizer=None, umap_n_neighbors=15, umap_n_components=5, umap_min_dist=0.0, umap_metric='cosine', vectorizer_ngram_range=(1, 2), stop_words=stop_words):\n",
    "    # Pré-processar os documentos\n",
    "    if clean_text:\n",
    "        docs = [clean_text(doc) for doc in docs]\n",
    "    \n",
    "    if not isinstance(stop_words, list):\n",
    "        stop_words = list(stop_words)\n",
    "    \n",
    "    # Configurações padrão do BERTopic\n",
    "    umap_model = UMAP(n_neighbors=umap_n_neighbors, n_components=umap_n_components, min_dist=umap_min_dist, metric=umap_metric)\n",
    "    vectorizer_model = CountVectorizer(ngram_range=vectorizer_ngram_range, stop_words=stop_words)\n",
    "    \n",
    "    # Check if we should use embeddings\n",
    "    if use_embeddings:\n",
    "        embeddings = precomputed_embeddings\n",
    "        if embeddings is None:\n",
    "            assert bert_model is not None, \"O modelo deve ser fornecido quando use_embeddings=True\"\n",
    "            assert tokenizer is not None, \"O tokenizador deve ser fornecido quando use_embeddings=True\"\n",
    "            \n",
    "            # Extrair embeddings para os documentos em lotes\n",
    "            embeddings = extract_embeddings(docs, bert_model, tokenizer)\n",
    "            \n",
    "            # Criar e treinar o modelo BERTopic usando os embeddings\n",
    "            topic_model = BERTopic(umap_model=umap_model, vectorizer_model=vectorizer_model)\n",
    "            topic_model.fit(docs, embeddings)\n",
    "    else:\n",
    "        # Criar e treinar o modelo BERTopic usando a configuração padrão\n",
    "        topic_model = BERTopic(umap_model=umap_model, vectorizer_model=vectorizer_model)\n",
    "        topic_model.fit(docs)\n",
    "    \n",
    "    return topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(PROJECT_ROOT, 'models')\n",
    "tokenizer_path = os.path.join(PROJECT_ROOT, 'tokenizers')\n",
    "model_name = 'bertimbal_model.pt'\n",
    "tokenizer_name= 'bertimbal_tokenizer'\n",
    "\n",
    "if not os.path.exists(f'{model_path}/{model_name}'):\n",
    "    trainer = train_model(\n",
    "        model=bert_model,\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=10,\n",
    "        model_save_dir=model_path,\n",
    "        model_name=model_name,\n",
    "        tokenizer_save_dir=tokenizer_name,\n",
    "        tokenizer_name=tokenizer_name\n",
    "    )\n",
    "\n",
    "    metrics = evaluate_model(trainer, test_dataset)\n",
    "    print(\"Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = load_model(model_path, model_name)\n",
    "tokenizer = load_tokenizer(tokenizer_path, tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertopic_name = f'{model_path}/bertopic_model.pt'\n",
    "\n",
    "if not os.path.exists(bertopic_name):\n",
    "    text_train = train_dataset['cleaned_text'].tolist()\n",
    "    bertopic_model = train_bertopic(docs=text_train)\n",
    "    bertopic_model.save(bertopic_name)\n",
    "\n",
    "bertopic_model = BERTopic.load(bertopic_name)\n",
    "bertopic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_name = f'{model_path}/roberta_embeddings.npy'\n",
    "if not os.path.exists(embeddings_name):\n",
    "    text_embeddings = train_dataset['cleaned_text'].tolist()\n",
    "    embeddings = extract_embeddings(text_embeddings, bert_model, tokenizer)\n",
    "    save_embeddings(embeddings, embeddings_name)\n",
    "\n",
    "embeddings = load_embeddings(embeddings_name)\n",
    "\n",
    "bertopic_roberta_embeddings_name = f'{model_path}/bertopic_roberta_embeddings_model.pt'\n",
    "if not os.path.exists(bertopic_roberta_embeddings_name):\n",
    "    text_train = train_dataset['cleaned_text'].tolist()\n",
    "    bertopic_roberta_embeddings_model = train_bertopic(docs=text_train, use_embeddings=True, precomputed_embeddings=embeddings)\n",
    "    bertopic_roberta_embeddings_model.save(bertopic_roberta_embeddings_name)\n",
    "\n",
    "bertopic_roberta_embeddings_model = BERTopic.load(bertopic_roberta_embeddings_name)\n",
    "bertopic_roberta_embeddings_model.visualize_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
