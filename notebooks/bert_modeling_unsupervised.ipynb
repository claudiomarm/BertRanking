{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\csouza\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\academic-fingerprint-4AictD0y-py3.11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importação de bibliotecas\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bertopic import BERTopic\n",
    "from transformers import BertTokenizer, BertForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments, BertModel\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicao da raiz do projeto\n",
    "\n",
    "PROJECT_ROOT = 'G:/Csouza/nlp/topic_modeling'\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "sys.path.insert(0, PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(extract_path, file_name='all_process.xlsx', sheet_name='Sheet1'):\n",
    "    \n",
    "    return pl.read_excel(f'{extract_path}/{file_name}', sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(PROJECT_ROOT, 'data', 'internal', 'fapesp_projects')\n",
    "\n",
    "full_data = extract(data_path)\n",
    "\n",
    "variables = {\n",
    "'N. Processo_B.V': 'n_processo',\n",
    "'Data de Início': 'data',\n",
    "'Título (Português)': 'titulo',\n",
    "'Grande Área do Conhecimento': 'grande_area',\n",
    "'Área do Conhecimento': 'area',\n",
    "'Subárea do Conhecimento': 'subarea',\n",
    "'Palavras-Chave do Processo': 'palavras_chave',\n",
    "'Assuntos': 'assuntos',\n",
    "'Resumo (Português)': 'resumo'}\n",
    "\n",
    "full_data = (\n",
    "    full_data\n",
    "    .lazy()\n",
    "    .rename(variables)\n",
    "    .select(variables.values())\n",
    "    .filter(\n",
    "        pl.col('n_processo').is_not_null(),\n",
    "        pl.col('resumo').is_not_null(),\n",
    "        pl.col('resumo') != '')\n",
    "    .with_columns(\n",
    "        pl.col('data').str.to_datetime('%m-%d-%y').dt.year().alias('ano'),\n",
    "        pl.col('data').str.to_datetime('%m-%d-%y').dt.month().alias('mes'))\n",
    "    .select(pl.exclude('data'))\n",
    ").collect()\n",
    "\n",
    "full_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_test = full_data.filter(pl.col('assuntos').is_not_null(), pl.col('area') == 'Medicina', pl.col('ano') >= 2020)\n",
    "\n",
    "data_train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_model(model='pt_core_news_sm'):\n",
    "    \"\"\"\n",
    "    Baixa o modelo de linguagem spaCy se não estiver presente.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nlp = spacy.load(model)\n",
    "    except OSError:\n",
    "        from spacy.cli import download\n",
    "        download(model)\n",
    "        nlp = spacy.load(model)\n",
    "    return nlp\n",
    "\n",
    "# Carregar o modelo de linguagem em português do spaCy\n",
    "nlp = get_spacy_model('pt_core_news_sm')\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(\"O argumento 'text' deve ser uma string.\")\n",
    "    \n",
    "    # Converter o texto para minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Processar o texto inteiro de uma vez\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lematizar os tokens (sem remover as stop words)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
    "    \n",
    "    # Unir tokens lematizados em uma string\n",
    "    cleaned_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Carregar os dados\n",
    "data = data_train_test.to_pandas()\n",
    "\n",
    "data['titulo'] = data['titulo'].astype(str)\n",
    "data['palavras_chave'] = data['palavras_chave'].astype(str)\n",
    "\n",
    "# Aplicar a limpeza de texto sem remover stop words\n",
    "data['cleaned_text'] = data['resumo'].apply(clean_text)\n",
    "data['cleaned_text'] += ' ' + data['titulo'].apply(clean_text) + ' ' + data['palavras_chave'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "bert_model = BertForMaskedLM.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "\n",
    "# Função de tokenização para MLM\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['cleaned_text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "dataset = Dataset.from_pandas(data)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Dividir em conjunto de treino e teste\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "# Data collator para MLM (vai automaticamente mascarar tokens)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_count'] = data['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "fig = px.histogram(data, x='word_count', nbins=30, title='Distribuição da Contagem de Palavras por Documento')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir as stop words em português usando spaCy\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "additional_stop_words = ['ser', 'de', 'se', 'o', 'para', 'em', 'além', 'este', 'esse', 'ao', 'do', 'pelo', 'por', 'au']\n",
    "stop_words |= set(additional_stop_words)\n",
    "stop_words = {word.lower() for word in stop_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join(data['cleaned_text']).split()\n",
    "all_words = [word for word in all_words if word not in stop_words]\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "common_words = word_freq.most_common(50)\n",
    "\n",
    "words, counts = zip(*common_words)\n",
    "word_freq_df = pd.DataFrame({'Palavra': words, 'Frequência': counts})\n",
    "\n",
    "fig = px.bar(word_freq_df, x='Frequência', y='Palavra', orientation='h', title='Top 10 Palavras Mais Frequentes')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(data['cleaned_text']).split()\n",
    "text = [word for word in all_words if word not in stop_words]\n",
    "\n",
    "text = ' '.join(text)\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color ='white').generate(text)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar a matriz de coocorrência com um limite de termos\n",
    "vectorizer = CountVectorizer(max_features=1000, min_df=5, max_df=0.8)\n",
    "X = vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "# Obter os nomes das palavras\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Calcular a matriz de coocorrência\n",
    "cooc_matrix = (X.T * X).tocoo()\n",
    "\n",
    "# Converter para DataFrame para melhor visualização\n",
    "cooc_df = pd.DataFrame.sparse.from_spmatrix(cooc_matrix, index=terms, columns=terms)\n",
    "\n",
    "# Filtrar para os termos mais frequentes\n",
    "filtered_cooc_df = cooc_df.loc[terms[:20], terms[:20]]\n",
    "\n",
    "# Visualizar a matriz de coocorrência\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(filtered_cooc_df, cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, tokenizer, train_dataset, test_dataset, data_collator, model_path, tokenizer_path, output_dir, overwrite_output_dir=True, save_steps=10_000, save_total_limit=2, prediction_loss_only=True, num_train_epochs=3, per_device_train_batch_size=8):    \n",
    "    \n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    os.makedirs(tokenizer_path, exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=overwrite_output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        save_steps=save_steps,\n",
    "        save_total_limit=save_total_limit,\n",
    "        prediction_loss_only=prediction_loss_only,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    # Treinar o modelo\n",
    "    trainer.train()\n",
    "\n",
    "    # Salvar o modelo e o tokenizer\n",
    "    if model_path:\n",
    "        trainer.save_model(model_path)\n",
    "    \n",
    "    if tokenizer_path:\n",
    "        tokenizer.save_pretrained(tokenizer_path)\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "def evaluate_model(trainer, test_dataset):\n",
    "    eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    loss = eval_results['eval_loss']\n",
    "    perplexity = np.exp(loss)\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'perplexity': perplexity\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def extract_embeddings(texts, model, tokenizer, max_length=512, batch_size=8):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        all_embeddings.append(batch_embeddings.cpu())\n",
    "    \n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    return all_embeddings\n",
    "\n",
    "def save_embeddings(embeddings, embeddings_path):\n",
    "    np.save(embeddings_path, embeddings)\n",
    "    \n",
    "def load_embeddings(embeddings_path):\n",
    "    if os.path.exists(embeddings_path):\n",
    "        return np.load(embeddings_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f'Embeddings file not found at {embeddings_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(PROJECT_ROOT, 'models')\n",
    "tokenizer_path = os.path.join(PROJECT_ROOT, 'tokenizers')\n",
    "results_path = os.path.join(model_path, 'results')\n",
    "\n",
    "embed_full_path = os.path.isfile(os.path.join(model_path, 'bertimbal_embeddings.npy'))\n",
    "tokzr_full_path = os.path.isfile(os.path.join(tokenizer_path, 'vocab.txt'))\n",
    "\n",
    "if not embed_full_path or not tokzr_full_path:\n",
    "    trainer = train_model(\n",
    "        model=bert_model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        data_collator=data_collator,\n",
    "        model_path=model_path,\n",
    "        tokenizer_path=tokenizer_path,\n",
    "        output_dir=results_path\n",
    "    )\n",
    "\n",
    "    metrics = evaluate_model(trainer, test_dataset)\n",
    "    print(\"Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "bert_model = BertModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_name = f'{model_path}/bertimbal_embeddings_09_09_24.npy'\n",
    "if not os.path.exists(embeddings_name):\n",
    "    text_embeddings = train_dataset['cleaned_text']\n",
    "    embeddings = extract_embeddings(text_embeddings, bert_model, tokenizer)\n",
    "    save_embeddings(embeddings, embeddings_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_path = os.path.join(PROJECT_ROOT, 'data', 'processed', 'usp_controlled_vocabulary')\n",
    "vocab_name = 'vocabulario_usp_hierarchy.json'\n",
    "\n",
    "# Carregar o vocabulário controlado da USP\n",
    "with open(f'{vocab_path}/{vocab_name}', 'r', encoding='utf-8') as file:\n",
    "    vocab_data = json.load(file)\n",
    "\n",
    "# Função para extrair termos de forma hierárquica\n",
    "def extract_terms(vocab_data, terms_list=None):\n",
    "    if terms_list is None:\n",
    "        terms_list = []\n",
    "\n",
    "    for item in vocab_data:\n",
    "        term = item['string']\n",
    "        terms_list.append(term)\n",
    "        if 'subterms' in item and item['subterms']:\n",
    "            extract_terms(item['subterms'], terms_list)\n",
    "\n",
    "    return terms_list\n",
    "\n",
    "# Extrair todos os termos do vocabulário controlado\n",
    "all_terms = extract_terms(vocab_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embedding.squeeze()\n",
    "\n",
    "def compute_similarity(embedding1, embedding2):\n",
    "    return cosine_similarity(embedding1.reshape(1, -1), embedding2.reshape(1, -1))[0, 0]\n",
    "\n",
    "# Obtenha os embeddings das palavras do dicionário\n",
    "dictionary_embeddings = np.array([get_embedding(word, tokenizer, bert_model) for word in all_terms])\n",
    "\n",
    "# Função para encontrar as 10 palavras mais semelhantes do dicionário para um texto\n",
    "def find_top_similar_words(text_embedding, dictionary_embeddings, dictionary_words, top_n=10):\n",
    "    # Calcular a similaridade entre o embedding do texto e os embeddings das palavras do dicionário\n",
    "    similarities = [compute_similarity(text_embedding, word_embedding) for word_embedding in dictionary_embeddings]\n",
    "    \n",
    "    # Obter os índices das top_n palavras mais semelhantes\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_n]\n",
    "    \n",
    "    # Retornar as palavras mais semelhantes\n",
    "    top_words = [dictionary_words[i] for i in top_indices]\n",
    "    return top_words\n",
    "\n",
    "# Aplicar aos textos\n",
    "top_words_per_text = []\n",
    "\n",
    "for text in data['cleaned_text']:\n",
    "    # Obter o embedding do texto\n",
    "    text_embedding = get_embedding(text, tokenizer, bert_model)\n",
    "    \n",
    "    # Encontrar as 10 palavras mais semelhantes do dicionário para o texto\n",
    "    top_words = find_top_similar_words(text_embedding, dictionary_embeddings, all_terms)\n",
    "    \n",
    "    top_words_per_text.append(top_words)\n",
    "\n",
    "# Resultado: Lista de top 10 palavras mais semelhantes para cada texto\n",
    "print(top_words_per_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
